{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "32b169b3",
   "metadata": {},
   "source": "# 🤖 Anthropic API Compatibility Examples\n\nThis notebook demonstrates how to use the Anthropic-compatible API endpoints provided by MLX Omni Server.\n\n## 📋 Overview\n\nMLX Omni Server provides fully compatible Anthropic Claude API endpoints, allowing you to use the Anthropic Python SDK to interact with locally running MLX models.\n\n### ✨ Key Features\n- **Model Listing**: Get available local MLX models\n- **Message Conversations**: Support for text generation and chat\n- **Thinking Mode**: Support for extended reasoning (Thinking)\n- **Tool Calling**: Support for streaming tool calls and function execution\n- **Real-time Streaming**: Support for streaming responses and incremental output\n\n### 🚀 Getting Started\nMake sure MLX Omni Server is running:\n```bash\nuv run uvicorn mlx_omni_server.main:app --reload --host 0.0.0.0 --port 10240\n```"
  },
  {
   "cell_type": "code",
   "id": "db5819fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T16:04:10.270354Z",
     "start_time": "2025-08-01T16:04:10.259723Z"
    }
   },
   "source": [
    "# Initialize Anthropic client\n",
    "import anthropic\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "# Configure client to use local MLX Omni Server\n",
    "client = anthropic.Anthropic(\n",
    "    base_url=\"http://localhost:10240/anthropic\",  # Local server endpoint\n",
    "    api_key=\"not-needed\",                         # API key not required for local server\n",
    "    auth_token=\"not-needed\"\n",
    ")\n",
    "\n",
    "print(\"✅ Anthropic client configured for MLX Omni Server\")\n",
    "print(\"🌐 Base URL: http://localhost:10240/anthropic\")\n",
    "print(\"🔑 API Key: Not required for local usage\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Anthropic client configured for MLX Omni Server\n",
      "🌐 Base URL: http://localhost:10240/anthropic\n",
      "🔑 API Key: Not required for local usage\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3bafa8f",
   "metadata": {},
   "source": "## 📋 Model Management - `/anthropic/v1/models`\n\nList all available MLX models that can be used with the Anthropic-compatible API.\n\n**API Reference**: [Anthropic Models API](https://docs.anthropic.com/en/api/models-list)"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d1349a27",
   "metadata": {},
   "source": "### 🔧 Testing with cURL\n\nYou can test the models endpoint directly using curl:\n\n```shell\ncurl http://localhost:10240/anthropic/v1/models \\\n     --header \"x-api-key: $ANTHROPIC_API_KEY\" \\\n     --header \"anthropic-version: 2023-06-01\"\n```\n\n### 🐍 Using Python SDK\n\nThe Anthropic Python SDK provides a seamless experience for accessing local models:"
  },
  {
   "cell_type": "code",
   "id": "280fe5e6-0c7f-4554-93a9-0d30cce21f2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T14:51:02.681478Z",
     "start_time": "2025-08-01T14:51:02.665628Z"
    }
   },
   "source": "# Get list of available models\nresponse = client.models.list(limit=20)\n\nprint(\"🎯 Available Models:\")\nprint(f\"📊 Total models found: {len(response.data)}\")\nprint(\"\\n🔍 First model details:\")\npprint(response.data[0].dict() if response.data else \"No models available\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Available Models:\n",
      "📊 Total models found: 20\n",
      "\n",
      "🔍 First model details:\n",
      "{'created_at': datetime.datetime(2025, 8, 1, 1, 53, 32, tzinfo=datetime.timezone.utc),\n",
      " 'display_name': 'mlx-community/Qwen3-Coder-30B-A3B-Instruct-4bit',\n",
      " 'id': 'mlx-community/Qwen3-Coder-30B-A3B-Instruct-4bit',\n",
      " 'type': 'model'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/07/bt1n4pzn5ln_b8ts86fztw9w0000gn/T/ipykernel_14114/4147969662.py:7: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  pprint(response.data[0].dict() if response.data else \"No models available\")\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "id": "539ec72e-a2ae-4abb-a485-6bbd8063bdbd",
   "metadata": {},
   "source": "# Display complete response structure\nprint(\"📋 Complete Models Response:\")\nprint(\"=\" * 50)\npprint(response.dict())\n\nprint(f\"\\n📝 Model Names:\")\nfor i, model in enumerate(response.data):\n    print(f\"  {i+1}. {model.id}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c95a5f88",
   "metadata": {},
   "source": "## 💬 Message API - `/anthropic/v1/messages`\n\nCreate conversations with MLX models using the Anthropic Messages API format.\n\n**API Reference**: [Anthropic Messages API](https://docs.anthropic.com/en/api/messages)"
  },
  {
   "cell_type": "markdown",
   "id": "e2764701",
   "metadata": {},
   "source": "### 🎭 Standard Text Generation\n\nBasic text generation without thinking mode enabled."
  },
  {
   "cell_type": "code",
   "id": "ea27212f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T15:00:19.089179Z",
     "start_time": "2025-08-01T15:00:17.206397Z"
    }
   },
   "source": "# Create a poetry message with system prompt\nmessage = client.messages.create(\n    model=\"mlx-community/gemma-3-1b-it-4bit-DWQ\",\n    max_tokens=1000,\n    temperature=1,\n    system=\"You are a world-class poet. Respond only with short poems.\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"text\",\n                    \"text\": \"Why is the ocean salty?\"\n                }\n            ]\n        }\n    ]\n)\n\nprint(\"🎨 Poetry Response Generated:\")\nprint(\"=\" * 40)\nprint(f\"📝 Model: {message.model}\")\nprint(f\"🆔 Message ID: {message.id}\")\nprint(f\"🔢 Input tokens: {message.usage.input_tokens}\")\nprint(f\"🔢 Output tokens: {message.usage.output_tokens}\")\nprint(f\"⚙️ Stop reason: {message.stop_reason}\")\nprint(\"\\n\" + \"=\" * 40)\n\nmessage",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎨 Poetry Response Generated:\n",
      "========================================\n",
      "📝 Model: mlx-community/gemma-3-1b-it-4bit-DWQ\n",
      "🆔 Message ID: msg_c3bd3ae930ab4a30bd0a17fc\n",
      "🔢 Input tokens: 31\n",
      "🔢 Output tokens: 52\n",
      "⚙️ Stop reason: end_turn\n",
      "\n",
      "========================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Message(id='msg_c3bd3ae930ab4a30bd0a17fc', content=[TextBlock(citations=None, text='The salt of ancient tears,\\nWhispers on currents, whispering fears.\\nA slow exhale, weight of stone,\\nRain spills down, a silver moan.\\n\\nThe waters trade, a shifting hue,\\nWhere minerals dance, forever new. \\n', type='text')], model='mlx-community/gemma-3-1b-it-4bit-DWQ', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(cache_creation_input_tokens=None, cache_read_input_tokens=None, input_tokens=31, output_tokens=52, server_tool_use=None, service_tier=None))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 47
  },
  {
   "cell_type": "code",
   "id": "4c291b46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T15:00:32.666470Z",
     "start_time": "2025-08-01T15:00:32.664492Z"
    }
   },
   "source": "# Extract and display the poem content\nprint(\"📖 Poem Content:\")\nprint(\"=\" * 30)\nfor block in message.content:\n    if block.type == \"text\":\n        print(f\"🎭 {block.text}\")\nprint(\"=\" * 30)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📖 Poem Content:\n",
      "==============================\n",
      "🎭 The salt of ancient tears,\n",
      "Whispers on currents, whispering fears.\n",
      "A slow exhale, weight of stone,\n",
      "Rain spills down, a silver moan.\n",
      "\n",
      "The waters trade, a shifting hue,\n",
      "Where minerals dance, forever new. \n",
      "\n",
      "==============================\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "cell_type": "code",
   "id": "86dac7fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T15:00:35.979287Z",
     "start_time": "2025-08-01T15:00:35.976886Z"
    }
   },
   "source": "# Display first content block details\nprint(\"🔍 First Content Block Analysis:\")\nprint(\"=\" * 35)\nfirst_block = message.content[0]\nprint(f\"📋 Type: {first_block.type}\")\nprint(f\"📝 Text: {first_block.text}\")\nprint(\"=\" * 35)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 First Content Block Analysis:\n",
      "===================================\n",
      "📋 Type: text\n",
      "📝 Text: The salt of ancient tears,\n",
      "Whispers on currents, whispering fears.\n",
      "A slow exhale, weight of stone,\n",
      "Rain spills down, a silver moan.\n",
      "\n",
      "The waters trade, a shifting hue,\n",
      "Where minerals dance, forever new. \n",
      "\n",
      "===================================\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "cell_type": "code",
   "id": "c052609e",
   "metadata": {},
   "source": "# Streaming conversation example\nprint(\"🌊 Starting Streaming Conversation:\")\nprint(\"=\" * 40)\nprint(\"🤖 Assistant: \", end=\"\", flush=True)\n\nwith client.messages.stream(\n    max_tokens=1024,\n    messages=[{\"role\": \"user\", \"content\": \"Hello! Tell me a fun fact about space.\"}],\n    model=\"mlx-community/gemma-3-1b-it-4bit-DWQ\",\n) as stream:\n    for text in stream.text_stream:\n        print(text, end=\"\", flush=True)\n\nprint(f\"\\n{'='*40}\")\nprint(\"✅ Streaming completed!\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "81144213",
   "metadata": {},
   "source": "### 🧠 Extended Thinking Mode\n\nEnable the model's internal reasoning process with Anthropic's extended thinking feature.\n\n**Documentation**: [Extended Thinking](https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking)"
  },
  {
   "cell_type": "code",
   "id": "1a7ef2c7",
   "metadata": {},
   "source": "# Select a model that supports thinking mode\nthinking_model = \"Qwen/Qwen3-0.6B-MLX-4bit\"\nprint(f\"🧠 Using thinking model: {thinking_model}\")\nprint(\"💭 Note: Thinking mode allows the model to show its reasoning process\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f0cdd801",
   "metadata": {},
   "source": "# Example: Mathematical reasoning with thinking mode\nresponse = client.messages.create(\n    model=thinking_model,\n    max_tokens=16000,\n    thinking={\n        \"type\": \"enabled\",\n        \"budget_tokens\": 10000  # Budget for thinking tokens (note: not fully implemented)\n    },\n    messages=[{\n        \"role\": \"user\",\n        \"content\": \"Are there an infinite number of prime numbers such that n mod 4 == 3?\"\n    }]\n)\n\nprint(\"🔢 Mathematical Reasoning Response:\")\nprint(\"=\" * 50)\n\n# Process and display the response blocks\nfor i, block in enumerate(response.content):\n    if block.type == \"thinking\":\n        print(f\"💭 Thinking Block {i+1}:\")\n        print(f\"   {block.thinking}\")\n        print()\n    elif block.type == \"text\":\n        print(f\"📝 Final Response:\")\n        print(f\"   {block.text}\")\n        print()\n\nprint(\"=\" * 50)\nprint(f\"📊 Usage: {response.usage.input_tokens} input + {response.usage.output_tokens} output tokens\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6064e62eb40ef08b",
   "metadata": {},
   "source": "# Streaming with thinking mode - detailed event monitoring\nprint(\"🌊 Streaming with Thinking Mode:\")\nprint(\"=\" * 45)\n\nwith client.messages.stream(\n    model=thinking_model,\n    max_tokens=16000,\n    thinking={\"type\": \"enabled\", \"budget_tokens\": 10000},\n    messages=[{\"role\": \"user\", \"content\": \"What is 27 * 453? Show your calculation steps.\"}],\n) as stream:\n    thinking_started = False\n    response_started = False\n\n    for event in stream:\n        if event.type == \"message_start\":\n            print(f\"🚀 Message started: {event.message.id}\")\n        elif event.type == \"content_block_start\":\n            if event.content_block.type == \"thinking\":\n                print(\"💭 Thinking process begins...\")\n                thinking_started = True\n            elif event.content_block.type == \"text\":\n                print(\"📝 Response output begins...\")\n                response_started = True\n        elif event.type == \"content_block_delta\":\n            if hasattr(event.delta, 'text') and response_started:\n                print(event.delta.text, end=\"\", flush=True)\n        elif event.type == \"message_stop\":\n            print(f\"\\n✅ Message completed\")\n            break\n\nprint(\"=\" * 45)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c59c93a087171d76",
   "metadata": {},
   "source": "### Streaming Tool Calls\n\nThis example demonstrates streaming tool calls using the Qwen3-30B model with fine-grained streaming of tool parameters."
  },
  {
   "cell_type": "code",
   "id": "1rkbnf8wsin",
   "source": "# Define comprehensive tools for streaming examples\ntools = [\n    {\n        \"name\": \"get_weather\",\n        \"description\": \"Get the current weather in a given location\",\n        \"input_schema\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"location\": {\n                    \"type\": \"string\",\n                    \"description\": \"The city and state, e.g. San Francisco, CA\"\n                },\n                \"unit\": {\n                    \"type\": \"string\",\n                    \"enum\": [\"celsius\", \"fahrenheit\"],\n                    \"description\": \"The temperature unit to use\"\n                }\n            },\n            \"required\": [\"location\"]\n        }\n    },\n    {\n        \"name\": \"send_email\",\n        \"description\": \"Send an email to a specified recipient\",\n        \"input_schema\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"to\": {\n                    \"type\": \"string\",\n                    \"description\": \"Email address of the recipient\"\n                },\n                \"subject\": {\n                    \"type\": \"string\",\n                    \"description\": \"Subject line of the email\"\n                },\n                \"body\": {\n                    \"type\": \"string\",\n                    \"description\": \"Content of the email\"\n                }\n            },\n            \"required\": [\"to\", \"subject\", \"body\"]\n        }\n    }\n]\n\n# Select a larger model for better tool calling performance\ntool_model = \"mlx-community/Qwen3-Coder-30B-A3B-Instruct-4bit\"\n\nprint(\"🔧 Tool Configuration:\")\nprint(\"=\" * 35)\nprint(f\"📋 Available tools: {len(tools)}\")\nfor i, tool in enumerate(tools):\n    print(f\"  {i+1}. {tool['name']}: {tool['description']}\")\nprint(f\"\\n🤖 Selected model: {tool_model}\")\nprint(\"📥 Note: Model will be downloaded automatically if not available locally\")\nprint(\"=\" * 35)",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T16:04:26.572342Z",
     "start_time": "2025-08-01T16:04:26.569528Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Tool Configuration:\n",
      "===================================\n",
      "📋 Available tools: 2\n",
      "  1. get_weather: Get the current weather in a given location\n",
      "  2. send_email: Send an email to a specified recipient\n",
      "\n",
      "🤖 Selected model: mlx-community/Qwen3-Coder-30B-A3B-Instruct-4bit\n",
      "📥 Note: Model will be downloaded automatically if not available locally\n",
      "===================================\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "cell_type": "markdown",
   "id": "4nl2kx7l2n",
   "source": "#### Basic Tool Call Example\n\nFirst, let's see a non-streaming tool call to understand the structure:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "x4mfx0xn00h",
   "source": "# Non-streaming tool call example for comparison\nprint(\"🔧 Non-Streaming Tool Call Example:\")\nprint(\"=\" * 45)\n\nresponse = client.messages.create(\n    model=tool_model,\n    max_tokens=10240,\n    tools=tools,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"What's the weather like in San Francisco? Also send an email to john@example.com about the meeting tomorrow.\"\n        }\n    ]\n)\n\nprint(f\"📊 Response Statistics:\")\nprint(f\"  🆔 Message ID: {response.id}\")\nprint(f\"  📈 Input tokens: {response.usage.input_tokens}\")\nprint(f\"  📉 Output tokens: {response.usage.output_tokens}\")\nprint(f\"  ⛔ Stop reason: {response.stop_reason}\")\n\nprint(f\"\\n📋 Content blocks ({len(response.content)}):\")\nfor i, block in enumerate(response.content):\n    print(f\"\\n  Block {i+1}: {block.type}\")\n    if block.type == \"text\":\n        print(f\"    📝 Text: {block.text}\")\n    elif block.type == \"tool_use\":\n        print(f\"    🔧 Tool: {block.name}\")\n        print(f\"    🆔 ID: {block.id}\")\n        print(f\"    📋 Parameters:\")\n        for key, value in block.input.items():\n            print(f\"      {key}: {value}\")\n\nprint(\"=\" * 45)",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-08-01T16:04:33.804128Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Non-Streaming Tool Call Example:\n",
      "=============================================\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1y2bitr4vio",
   "source": "#### Streaming Tool Calls with Fine-Grained Parameter Parsing\n\nNow let's see the streaming version, which shows the tool parameters being built incrementally:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "4uncumlxnaj",
   "source": "# Advanced streaming tool call example with detailed event logging\nprint(\"🌊 Advanced Streaming Tool Call Analysis:\")\nprint(\"=\" * 55)\nprint(\"This example shows fine-grained streaming of tool parameters\")\nprint(\"=\" * 55)\n\nwith client.messages.stream(\n    model=tool_model,\n    max_tokens=10240,\n    tools=tools,\n    messages=[\n        {\n            \"role\": \"user\", \n            \"content\": \"Check the weather in New York City and send an email to alice@company.com with the subject 'Weather Update' and tell her about the weather.\"\n        }\n    ]\n) as stream:\n    \n    current_content = \"\"\n    tool_calls = {}\n    event_count = 0\n    \n    for event in stream:\n        event_count += 1\n        print(f\"\\n📅 Event #{event_count}: {event.type}\")\n        \n        if event.type == \"message_start\":\n            print(f\"   🚀 Message ID: {event.message.id}\")\n            \n        elif event.type == \"content_block_start\":\n            print(f\"   🎬 Block {event.index}: {event.content_block.type}\")\n            if event.content_block.type == \"tool_use\":\n                tool_id = event.content_block.id\n                tool_name = event.content_block.name\n                tool_calls[tool_id] = {\n                    \"name\": tool_name,\n                    \"partial_input\": \"\",\n                    \"final_input\": {}\n                }\n                print(f\"   🔧 Tool: {tool_name} (ID: {tool_id[:8]}...)\")\n                \n        elif event.type == \"content_block_delta\":\n            if event.delta.type == \"text_delta\":\n                current_content += event.delta.text\n                print(f\"   📝 Text: '{event.delta.text}'\")\n                \n            elif event.delta.type == \"input_json_delta\":\n                partial_json = event.delta.partial_json\n                print(f\"   🧩 JSON fragment: '{partial_json}'\")\n                \n                # Update tool input buffer\n                for tool_id, tool_info in tool_calls.items():\n                    tool_info[\"partial_input\"] += partial_json\n                    print(f\"   📊 Buffer for {tool_info['name']}: '{tool_info['partial_input']}'\")\n                    \n                    # Attempt to parse accumulated JSON\n                    try:\n                        parsed = json.loads(tool_info[\"partial_input\"])\n                        tool_info[\"final_input\"] = parsed\n                        print(f\"   ✅ Parsed successfully: {parsed}\")\n                    except json.JSONDecodeError:\n                        print(f\"   ⏳ JSON incomplete, continuing...\")\n                        \n        elif event.type == \"content_block_stop\":\n            print(f\"   🏁 Block {event.index} completed\")\n            \n        elif event.type == \"message_stop\":\n            print(f\"   🔚 Message finished\")\n\nprint(f\"\\n{'='*55}\")\nprint(\"📊 FINAL RESULTS:\")\nprint(f\"📝 Complete text response: '{current_content}'\")\nprint(f\"🔧 Tool calls executed: {len(tool_calls)}\")\nfor tool_id, tool_info in tool_calls.items():\n    print(f\"  • {tool_info['name']} (ID: {tool_id[:8]}...)\")\n    print(f\"    Parameters: {tool_info['final_input']}\")\nprint(\"=\"*55)",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T15:42:51.161688Z",
     "start_time": "2025-08-01T15:42:41.877896Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌊 Advanced Streaming Tool Call Analysis:\n",
      "=======================================================\n",
      "This example shows fine-grained streaming of tool parameters\n",
      "=======================================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[53]\u001B[39m\u001B[32m, line 7\u001B[39m\n\u001B[32m      4\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mThis example shows fine-grained streaming of tool parameters\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m      5\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33m=\u001B[39m\u001B[33m\"\u001B[39m * \u001B[32m55\u001B[39m)\n\u001B[32m----> \u001B[39m\u001B[32m7\u001B[39m \u001B[38;5;28;43;01mwith\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mclient\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstream\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m      8\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtool_model\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      9\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmax_tokens\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m10240\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     10\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtools\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtools\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     11\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m=\u001B[49m\u001B[43m[\u001B[49m\n\u001B[32m     12\u001B[39m \u001B[43m        \u001B[49m\u001B[43m{\u001B[49m\n\u001B[32m     13\u001B[39m \u001B[43m            \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mrole\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43muser\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[32m     14\u001B[39m \u001B[43m            \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcontent\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mCheck the weather in New York City and send an email to alice@company.com with the subject \u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mWeather Update\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m and tell her about the weather.\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\n\u001B[32m     15\u001B[39m \u001B[43m        \u001B[49m\u001B[43m}\u001B[49m\n\u001B[32m     16\u001B[39m \u001B[43m    \u001B[49m\u001B[43m]\u001B[49m\n\u001B[32m     17\u001B[39m \u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mas\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m     19\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcurrent_content\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\n\u001B[32m     20\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtool_calls\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[43m}\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/workspace/mlx-omni-server/.venv/lib/python3.12/site-packages/anthropic/lib/streaming/_messages.py:154\u001B[39m, in \u001B[36mMessageStreamManager.__enter__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    153\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__enter__\u001B[39m(\u001B[38;5;28mself\u001B[39m) -> MessageStream:\n\u001B[32m--> \u001B[39m\u001B[32m154\u001B[39m     raw_stream = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m__api_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    155\u001B[39m     \u001B[38;5;28mself\u001B[39m.__stream = MessageStream(raw_stream)\n\u001B[32m    156\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.__stream\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/workspace/mlx-omni-server/.venv/lib/python3.12/site-packages/anthropic/_base_client.py:1314\u001B[39m, in \u001B[36mSyncAPIClient.post\u001B[39m\u001B[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001B[39m\n\u001B[32m   1300\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mpost\u001B[39m(\n\u001B[32m   1301\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   1302\u001B[39m     path: \u001B[38;5;28mstr\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1309\u001B[39m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] | \u001B[38;5;28;01mNone\u001B[39;00m = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   1310\u001B[39m ) -> ResponseT | _StreamT:\n\u001B[32m   1311\u001B[39m     opts = FinalRequestOptions.construct(\n\u001B[32m   1312\u001B[39m         method=\u001B[33m\"\u001B[39m\u001B[33mpost\u001B[39m\u001B[33m\"\u001B[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001B[32m   1313\u001B[39m     )\n\u001B[32m-> \u001B[39m\u001B[32m1314\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(ResponseT, \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/workspace/mlx-omni-server/.venv/lib/python3.12/site-packages/anthropic/_base_client.py:1037\u001B[39m, in \u001B[36mSyncAPIClient.request\u001B[39m\u001B[34m(self, cast_to, options, stream, stream_cls)\u001B[39m\n\u001B[32m   1035\u001B[39m response = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1036\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1037\u001B[39m     response = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_client\u001B[49m\u001B[43m.\u001B[49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1038\u001B[39m \u001B[43m        \u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1039\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_should_stream_response_body\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1040\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1041\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1042\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m httpx.TimeoutException \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[32m   1043\u001B[39m     log.debug(\u001B[33m\"\u001B[39m\u001B[33mEncountered httpx.TimeoutException\u001B[39m\u001B[33m\"\u001B[39m, exc_info=\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/workspace/mlx-omni-server/.venv/lib/python3.12/site-packages/httpx/_client.py:926\u001B[39m, in \u001B[36mClient.send\u001B[39m\u001B[34m(self, request, stream, auth, follow_redirects)\u001B[39m\n\u001B[32m    922\u001B[39m \u001B[38;5;28mself\u001B[39m._set_timeout(request)\n\u001B[32m    924\u001B[39m auth = \u001B[38;5;28mself\u001B[39m._build_request_auth(request, auth)\n\u001B[32m--> \u001B[39m\u001B[32m926\u001B[39m response = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_send_handling_auth\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    927\u001B[39m \u001B[43m    \u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    928\u001B[39m \u001B[43m    \u001B[49m\u001B[43mauth\u001B[49m\u001B[43m=\u001B[49m\u001B[43mauth\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    929\u001B[39m \u001B[43m    \u001B[49m\u001B[43mfollow_redirects\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfollow_redirects\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    930\u001B[39m \u001B[43m    \u001B[49m\u001B[43mhistory\u001B[49m\u001B[43m=\u001B[49m\u001B[43m[\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    931\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    932\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    933\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m stream:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/workspace/mlx-omni-server/.venv/lib/python3.12/site-packages/httpx/_client.py:954\u001B[39m, in \u001B[36mClient._send_handling_auth\u001B[39m\u001B[34m(self, request, auth, follow_redirects, history)\u001B[39m\n\u001B[32m    951\u001B[39m request = \u001B[38;5;28mnext\u001B[39m(auth_flow)\n\u001B[32m    953\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m954\u001B[39m     response = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_send_handling_redirects\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    955\u001B[39m \u001B[43m        \u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    956\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfollow_redirects\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfollow_redirects\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    957\u001B[39m \u001B[43m        \u001B[49m\u001B[43mhistory\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhistory\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    958\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    959\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    960\u001B[39m         \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/workspace/mlx-omni-server/.venv/lib/python3.12/site-packages/httpx/_client.py:991\u001B[39m, in \u001B[36mClient._send_handling_redirects\u001B[39m\u001B[34m(self, request, follow_redirects, history)\u001B[39m\n\u001B[32m    988\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m hook \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m._event_hooks[\u001B[33m\"\u001B[39m\u001B[33mrequest\u001B[39m\u001B[33m\"\u001B[39m]:\n\u001B[32m    989\u001B[39m     hook(request)\n\u001B[32m--> \u001B[39m\u001B[32m991\u001B[39m response = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_send_single_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    992\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    993\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m hook \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m._event_hooks[\u001B[33m\"\u001B[39m\u001B[33mresponse\u001B[39m\u001B[33m\"\u001B[39m]:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/workspace/mlx-omni-server/.venv/lib/python3.12/site-packages/httpx/_client.py:1027\u001B[39m, in \u001B[36mClient._send_single_request\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m   1022\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[32m   1023\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mAttempted to send an async request with a sync Client instance.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1024\u001B[39m     )\n\u001B[32m   1026\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m request_context(request=request):\n\u001B[32m-> \u001B[39m\u001B[32m1027\u001B[39m     response = \u001B[43mtransport\u001B[49m\u001B[43m.\u001B[49m\u001B[43mhandle_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1029\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response.stream, SyncByteStream)\n\u001B[32m   1031\u001B[39m response.request = request\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/workspace/mlx-omni-server/.venv/lib/python3.12/site-packages/httpx/_transports/default.py:236\u001B[39m, in \u001B[36mHTTPTransport.handle_request\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m    223\u001B[39m req = httpcore.Request(\n\u001B[32m    224\u001B[39m     method=request.method,\n\u001B[32m    225\u001B[39m     url=httpcore.URL(\n\u001B[32m   (...)\u001B[39m\u001B[32m    233\u001B[39m     extensions=request.extensions,\n\u001B[32m    234\u001B[39m )\n\u001B[32m    235\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m map_httpcore_exceptions():\n\u001B[32m--> \u001B[39m\u001B[32m236\u001B[39m     resp = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_pool\u001B[49m\u001B[43m.\u001B[49m\u001B[43mhandle_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreq\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    238\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(resp.stream, typing.Iterable)\n\u001B[32m    240\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m Response(\n\u001B[32m    241\u001B[39m     status_code=resp.status,\n\u001B[32m    242\u001B[39m     headers=resp.headers,\n\u001B[32m    243\u001B[39m     stream=ResponseStream(resp.stream),\n\u001B[32m    244\u001B[39m     extensions=resp.extensions,\n\u001B[32m    245\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/workspace/mlx-omni-server/.venv/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:256\u001B[39m, in \u001B[36mConnectionPool.handle_request\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m    253\u001B[39m         closing = \u001B[38;5;28mself\u001B[39m._assign_requests_to_connections()\n\u001B[32m    255\u001B[39m     \u001B[38;5;28mself\u001B[39m._close_connections(closing)\n\u001B[32m--> \u001B[39m\u001B[32m256\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m exc \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    258\u001B[39m \u001B[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001B[39;00m\n\u001B[32m    259\u001B[39m \u001B[38;5;66;03m# the point at which the response is closed.\u001B[39;00m\n\u001B[32m    260\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response.stream, typing.Iterable)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/workspace/mlx-omni-server/.venv/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:236\u001B[39m, in \u001B[36mConnectionPool.handle_request\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m    232\u001B[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001B[32m    234\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    235\u001B[39m     \u001B[38;5;66;03m# Send the request on the assigned connection.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m236\u001B[39m     response = \u001B[43mconnection\u001B[49m\u001B[43m.\u001B[49m\u001B[43mhandle_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    237\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpool_request\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrequest\u001B[49m\n\u001B[32m    238\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    239\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m ConnectionNotAvailable:\n\u001B[32m    240\u001B[39m     \u001B[38;5;66;03m# In some cases a connection may initially be available to\u001B[39;00m\n\u001B[32m    241\u001B[39m     \u001B[38;5;66;03m# handle a request, but then become unavailable.\u001B[39;00m\n\u001B[32m    242\u001B[39m     \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[32m    243\u001B[39m     \u001B[38;5;66;03m# In this case we clear the connection and try again.\u001B[39;00m\n\u001B[32m    244\u001B[39m     pool_request.clear_connection()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/workspace/mlx-omni-server/.venv/lib/python3.12/site-packages/httpcore/_sync/http_proxy.py:206\u001B[39m, in \u001B[36mForwardHTTPConnection.handle_request\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m    193\u001B[39m url = URL(\n\u001B[32m    194\u001B[39m     scheme=\u001B[38;5;28mself\u001B[39m._proxy_origin.scheme,\n\u001B[32m    195\u001B[39m     host=\u001B[38;5;28mself\u001B[39m._proxy_origin.host,\n\u001B[32m    196\u001B[39m     port=\u001B[38;5;28mself\u001B[39m._proxy_origin.port,\n\u001B[32m    197\u001B[39m     target=\u001B[38;5;28mbytes\u001B[39m(request.url),\n\u001B[32m    198\u001B[39m )\n\u001B[32m    199\u001B[39m proxy_request = Request(\n\u001B[32m    200\u001B[39m     method=request.method,\n\u001B[32m    201\u001B[39m     url=url,\n\u001B[32m   (...)\u001B[39m\u001B[32m    204\u001B[39m     extensions=request.extensions,\n\u001B[32m    205\u001B[39m )\n\u001B[32m--> \u001B[39m\u001B[32m206\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_connection\u001B[49m\u001B[43m.\u001B[49m\u001B[43mhandle_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mproxy_request\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/workspace/mlx-omni-server/.venv/lib/python3.12/site-packages/httpcore/_sync/connection.py:103\u001B[39m, in \u001B[36mHTTPConnection.handle_request\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m    100\u001B[39m     \u001B[38;5;28mself\u001B[39m._connect_failed = \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m    101\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m exc\n\u001B[32m--> \u001B[39m\u001B[32m103\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_connection\u001B[49m\u001B[43m.\u001B[49m\u001B[43mhandle_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/workspace/mlx-omni-server/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:136\u001B[39m, in \u001B[36mHTTP11Connection.handle_request\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m    134\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m Trace(\u001B[33m\"\u001B[39m\u001B[33mresponse_closed\u001B[39m\u001B[33m\"\u001B[39m, logger, request) \u001B[38;5;28;01mas\u001B[39;00m trace:\n\u001B[32m    135\u001B[39m         \u001B[38;5;28mself\u001B[39m._response_closed()\n\u001B[32m--> \u001B[39m\u001B[32m136\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m exc\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/workspace/mlx-omni-server/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:106\u001B[39m, in \u001B[36mHTTP11Connection.handle_request\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m     95\u001B[39m     \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[32m     97\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m Trace(\n\u001B[32m     98\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mreceive_response_headers\u001B[39m\u001B[33m\"\u001B[39m, logger, request, kwargs\n\u001B[32m     99\u001B[39m ) \u001B[38;5;28;01mas\u001B[39;00m trace:\n\u001B[32m    100\u001B[39m     (\n\u001B[32m    101\u001B[39m         http_version,\n\u001B[32m    102\u001B[39m         status,\n\u001B[32m    103\u001B[39m         reason_phrase,\n\u001B[32m    104\u001B[39m         headers,\n\u001B[32m    105\u001B[39m         trailing_data,\n\u001B[32m--> \u001B[39m\u001B[32m106\u001B[39m     ) = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_receive_response_headers\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    107\u001B[39m     trace.return_value = (\n\u001B[32m    108\u001B[39m         http_version,\n\u001B[32m    109\u001B[39m         status,\n\u001B[32m    110\u001B[39m         reason_phrase,\n\u001B[32m    111\u001B[39m         headers,\n\u001B[32m    112\u001B[39m     )\n\u001B[32m    114\u001B[39m network_stream = \u001B[38;5;28mself\u001B[39m._network_stream\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/workspace/mlx-omni-server/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:177\u001B[39m, in \u001B[36mHTTP11Connection._receive_response_headers\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m    174\u001B[39m timeout = timeouts.get(\u001B[33m\"\u001B[39m\u001B[33mread\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[32m    176\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m177\u001B[39m     event = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_receive_event\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    178\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(event, h11.Response):\n\u001B[32m    179\u001B[39m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/workspace/mlx-omni-server/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:217\u001B[39m, in \u001B[36mHTTP11Connection._receive_event\u001B[39m\u001B[34m(self, timeout)\u001B[39m\n\u001B[32m    214\u001B[39m     event = \u001B[38;5;28mself\u001B[39m._h11_state.next_event()\n\u001B[32m    216\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m event \u001B[38;5;129;01mis\u001B[39;00m h11.NEED_DATA:\n\u001B[32m--> \u001B[39m\u001B[32m217\u001B[39m     data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_network_stream\u001B[49m\u001B[43m.\u001B[49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    218\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mREAD_NUM_BYTES\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtimeout\u001B[49m\n\u001B[32m    219\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    221\u001B[39m     \u001B[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001B[39;00m\n\u001B[32m    222\u001B[39m     \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[32m    223\u001B[39m     \u001B[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    227\u001B[39m     \u001B[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001B[39;00m\n\u001B[32m    228\u001B[39m     \u001B[38;5;66;03m# it as a ConnectError.\u001B[39;00m\n\u001B[32m    229\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m data == \u001B[33mb\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/workspace/mlx-omni-server/.venv/lib/python3.12/site-packages/httpcore/_backends/sync.py:128\u001B[39m, in \u001B[36mSyncStream.read\u001B[39m\u001B[34m(self, max_bytes, timeout)\u001B[39m\n\u001B[32m    126\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m map_exceptions(exc_map):\n\u001B[32m    127\u001B[39m     \u001B[38;5;28mself\u001B[39m._sock.settimeout(timeout)\n\u001B[32m--> \u001B[39m\u001B[32m128\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_sock\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrecv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmax_bytes\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "cell_type": "markdown",
   "id": "etz6rqi9wlw",
   "source": "#### Simplified Streaming Tool Call Handler\n\nHere's a cleaner example that focuses on just the tool call results:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "lfx8kaaxvy",
   "source": "# Simplified streaming tool call handler for practical use\ndef stream_with_tools(user_message, show_details=False):\n    \"\"\"Stream a message with tools and display results cleanly.\"\"\"\n    \n    print(f\"👤 User: {user_message}\")\n    print(\"🔄 Streaming response...\")\n    print(\"=\" * 50)\n    \n    with client.messages.stream(\n        model=tool_model,\n        max_tokens=1024,\n        tools=tools,\n        messages=[{\"role\": \"user\", \"content\": user_message}]\n    ) as stream:\n        \n        text_content = \"\"\n        current_tool = None\n        tool_input_buffer = \"\"\n        tool_results = []\n        \n        for event in stream:\n            if event.type == \"content_block_start\":\n                if event.content_block.type == \"text\":\n                    print(\"🤖 Assistant: \", end=\"\", flush=True)\n                elif event.content_block.type == \"tool_use\":\n                    current_tool = {\n                        \"name\": event.content_block.name,\n                        \"id\": event.content_block.id\n                    }\n                    tool_input_buffer = \"\"\n                    print(f\"\\n🔧 Invoking tool: {current_tool['name']}\")\n                    \n            elif event.type == \"content_block_delta\":\n                if event.delta.type == \"text_delta\":\n                    text_content += event.delta.text\n                    print(event.delta.text, end=\"\", flush=True)\n                elif event.delta.type == \"input_json_delta\":\n                    tool_input_buffer += event.delta.partial_json\n                    if show_details:\n                        print(f\"   📝 Building parameters: {tool_input_buffer}\", end=\"\\r\")\n                    \n            elif event.type == \"content_block_stop\":\n                if current_tool:\n                    try:\n                        parsed_input = json.loads(tool_input_buffer)\n                        tool_results.append({\n                            \"name\": current_tool[\"name\"],\n                            \"id\": current_tool[\"id\"][:8] + \"...\",\n                            \"parameters\": parsed_input\n                        })\n                        print(f\"   ✅ Parameters: {parsed_input}\")\n                    except json.JSONDecodeError:\n                        print(f\"   ❌ Invalid JSON: {tool_input_buffer}\")\n                    current_tool = None\n                    tool_input_buffer = \"\"\n                else:\n                    print()  # End text line\n                    \n    print(\"=\" * 50)\n    print(\"📊 Summary:\")\n    if text_content:\n        print(f\"📝 Text response: {len(text_content)} characters\")\n    print(f\"🔧 Tools called: {len(tool_results)}\")\n    for tool in tool_results:\n        print(f\"  • {tool['name']} ({tool['id']}) with {len(tool['parameters'])} parameters\")\n    print(\"✅ Stream completed!\\n\")\n\n# Test with single tool call\nstream_with_tools(\"Get the weather for Tokyo, Japan in celsius\")",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8v8os1b04yn",
   "source": "# Test multiple tool calls in one request with detailed monitoring\nstream_with_tools(\n    \"Send an email to team@company.com about the quarterly results meeting next Friday, and also check the weather in London\",\n    show_details=True  # Show parameter building process\n)",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b7aob1l3vbr",
   "source": "### 📚 Understanding Streaming Tool Call Architecture\n\nThe streaming tool call implementation follows the Anthropic specification with several key components:\n\n#### 🔄 Event Flow\n1. **`content_block_start`** - Announces a new `tool_use` block with tool name and ID\n2. **`content_block_delta`** with `input_json_delta` - Streams partial JSON fragments \n3. **`content_block_stop`** - Signals tool call completion\n\n#### ⚡ Key Benefits\n- **Real-time Feedback**: See tool parameters being constructed incrementally\n- **Partial JSON Handling**: Robust parsing of incomplete JSON during streaming  \n- **Multiple Tool Support**: Each tool gets separate content blocks with independent streaming\n- **Fine-grained Control**: Parameters stream incrementally rather than all-at-once\n\n#### 🛠️ Technical Implementation\n- Uses **HuggingFace tool parser** with Qwen3-inspired incremental JSON building\n- Provides **smooth streaming** of tool parameters without blocking\n- Supports **complex nested parameters** and multiple simultaneous tool calls\n- Includes **error handling** for malformed JSON during streaming\n\n#### 🎯 Use Cases\n- **Interactive Applications**: Show users what tools are being called in real-time\n- **Debugging**: Monitor tool parameter construction for troubleshooting\n- **User Experience**: Provide immediate feedback during long-running tool operations\n- **Development**: Understand model behavior during tool selection and parameter generation\n\nThis implementation demonstrates MLX Omni Server's capability to provide streaming tool calls compatible with the Anthropic API specification while leveraging local MLX models for inference.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
